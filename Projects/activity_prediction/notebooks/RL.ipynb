{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405729df-b263-4def-9d18-a5600b030958",
   "metadata": {},
   "source": [
    "# In this notebook I am experimenting with combining MCMC sampling with activity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23eb5f0e-cced-4cbc-9268-76e582d036ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../../../src/')\n",
    "sys.path.append('../../activity_prediction/')\n",
    "import proteusAI.ML.plm.esm_tools as esm_tools\n",
    "import random\n",
    "from activity_predictor import FFNN\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "alphabet = esm_tools.alphabet.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26591021-06c3-4f81-99b7-5da1843d712e",
   "metadata": {},
   "source": [
    "## Dummy workflow for RL\n",
    "---\n",
    "Define position picker and mutant picker network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3888daac-6cc6-40e5-a262-0ab056a2dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, initial_seq, mut_depth=3, max_len=1024, aa_num=20, batch_size=10, mutation_penalty=-10, models_path='../../activity_prediction/checkpoints/'):\n",
    "        self.initial_seq = initial_seq\n",
    "        self.mut_depth = mut_depth\n",
    "        self.counter = 0\n",
    "        self.max_len = max_len\n",
    "        self.aa_num = aa_num\n",
    "        self.batch_size = batch_size\n",
    "        self.mutation_penalty = mutation_penalty\n",
    "        \n",
    "        self.AAs = ('A', 'C', 'D', 'E', 'F', 'G', 'H',\n",
    "                    'I', 'K', 'L', 'M', 'N', 'P', 'Q',\n",
    "                    'R', 'S', 'T', 'V', 'W', 'Y')\n",
    "        self.seqs = [self.initial_seq] * self.batch_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.ensemble = []\n",
    "        self.models_path = models_path\n",
    "        for m in os.listdir(models_path):\n",
    "            if 'model' in m:\n",
    "                model = FFNN(input_size=1280, output_size=1, hidden_layers=[1280, 1280], dropout_rate=0.2)\n",
    "                model.load_state_dict(torch.load(os.path.join(models_path, m)))\n",
    "                model.to(self.device)\n",
    "                model.eval()\n",
    "                self.ensemble.append(model)\n",
    "\n",
    "        self.s0 = self.compute_state(self.seqs)\n",
    "        # Initialize the environment\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        return self.s0\n",
    "\n",
    "    def step(self, a):\n",
    "        # Process the actions vector\n",
    "        self.seqs, penalties = self.mutate(a)\n",
    "\n",
    "        # Compute sequence representations (states)\n",
    "        seq_reps = self.compute_state(self.seqs)\n",
    "\n",
    "        # Compute activity values as rewards\n",
    "        rewards = self.dummy_activity_value(seq_reps) - penalties\n",
    "\n",
    "        # break if max mut depth has been reached.\n",
    "        self.counter += 1\n",
    "        done = False\n",
    "        if self.counter == self.mut_depth:\n",
    "            done = True\n",
    "        \n",
    "        return seq_reps, rewards, done, {} # {} could be information\n",
    "\n",
    "    def mutate(self, a):\n",
    "        seqs = self.seqs\n",
    "        # You should provide the implementation of this method\n",
    "        penalties = torch.zeros(len(seqs))\n",
    "        for i, seq in enumerate(seqs):\n",
    "            max_len = len(seq)\n",
    "            pos, mut = self.get_action(a[i])\n",
    "            \n",
    "            # penalize predictions outside the sequence length or missense mutations\n",
    "            if pos > max_len:\n",
    "                penalties[i] = self.mutation_penalty\n",
    "            elif mut == seq[pos]:\n",
    "                penalties[i] = self.mutation_penalty\n",
    "            else:\n",
    "                mut_seq = seq[:pos] + mut + seq[pos+1:]\n",
    "                seqs[i] = mut_seq\n",
    "        \n",
    "        self.seqs = seqs\n",
    "        return seqs, penalties\n",
    "    \n",
    "    def compute_state(self, seqs):\n",
    "        results, batch_lens, batch_labels, _ = esm_tools.esm_compute(seqs)\n",
    "        r = esm_tools.get_seq_rep(results, batch_lens)\n",
    "        return torch.stack(r)\n",
    "\n",
    "    def dummy_activity_value(self, seq_reps):\n",
    "        activities = []\n",
    "        for model in self.ensemble:\n",
    "            act = model(seq_reps.to(self.device))\n",
    "            activities.append(act.detach().cpu().numpy())\n",
    "\n",
    "        average_activity = np.mean(activities, axis=0) \n",
    "        return torch.from_numpy(average_activity)  \n",
    "\n",
    "\n",
    "    def get_action(self, a):\n",
    "        # calculate position\n",
    "        pos = a // len(self.AAs)\n",
    "\n",
    "        # calculate mutation index\n",
    "        mut_index = a % len(self.AAs)\n",
    "\n",
    "        # get the corresponding residue\n",
    "        mut = self.AAs[mut_index]\n",
    "        return pos, mut\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, dropout_rate):\n",
    "        super(QNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def epsilongreedy(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return torch.randint(low=0, high=self.output_size, size=(state.size(0),))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self(state), dim=1)\n",
    "\n",
    "def train_qlearn(environment, Qnet, alpha=0.001, gamma=0.9, epsilon=0.05, max_epochs=10000):\n",
    "    optimizer = torch.optim.Adam(Qnet.parameters(), lr=alpha)\n",
    "    max_reward = float('-inf')\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        s = environment.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            a = Qnet.epsilongreedy(s, epsilon)\n",
    "            s_next, r, done, _ = environment.step(a)\n",
    "\n",
    "            target = r + gamma * torch.max(Qnet(s_next))\n",
    "            output = Qnet(s)[torch.arange(s.size(0)), a]\n",
    "\n",
    "            total_reward += r.sum().item()\n",
    "\n",
    "            loss = (target - output).pow(2).sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                s = torch.Tensor(s_next)\n",
    "\n",
    "        # Save the model if it has the highest total reward so far\n",
    "        if total_reward > max_reward:\n",
    "            max_reward = total_reward\n",
    "            torch.save(Qnet.state_dict(), \"best_model.pth\")\n",
    "            \n",
    "        if epoch == 1:\n",
    "            break\n",
    "\n",
    "    return Qnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "311e6445-1708-4278-8693-39c0b3f3f466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=1280, out_features=20480, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"MGSSEDQAYRLLNDYAN\"\n",
    "model_dim = 1280\n",
    "max_seq_len = 1024\n",
    "num_residues = 20\n",
    "action_space = max_seq_len * num_residues\n",
    "\n",
    "sum_sq = 0\n",
    "environment = Environment(seq, batch_size=10)\n",
    "Qnet = QNet(model_dim, action_space, [model_dim, model_dim], 0.2)\n",
    "\n",
    "train_qlearn(environment, Qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bd180-8ee0-4859-bcf6-33893b93435b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
